# üß† Roteiro de treinamento para o Exerc√≠cio

## 1. An√°lise de Dados

### 1.1 Valores Vazios
- Verifique se existem dados faltantes na base.
- Poss√≠veis solu√ß√µes:
  - **Remover registros** ‚Üí se for uma parcela pequena (~at√© 10%).
  - **Preencher valores num√©ricos** ‚Üí m√©dia ou mediana (mais robusta a outliers).
  - **Preencher valores categ√≥ricos** ‚Üí moda (valor mais frequente).

### 1.2 Valores Inconsistentes
- Exemplo:
  - Correto: `"Alto"`
  - Incorreto: `"Auto"`
- Corrija via scripts de padroniza√ß√£o em Python, sem precisar editar manualmente cada caso.

### 1.3 Outliers
- Como identificar:
  - Gr√°ficos: scatter, boxplot ou histograma.
  - Estat√≠stica: **Intervalo Interquartil (IQR)**.
- Observa√ß√£o: nem sempre outliers s√£o erros; √†s vezes representam casos especiais (ex.: clientes VIP).

---

## 2. Pr√©-Processamento de Dados

### 2.1 Codifica√ß√£o de Vari√°veis Categ√≥ricas
- **LabelEncoder** ‚Üí para vari√°veis categ√≥ricas simples.
- **OneHotEncoder** ‚Üí vari√°veis **nominais** (sem ordem).
- **OrdinalEncoder** ‚Üí vari√°veis **ordinais** (com ordem, ex.: Baixo < M√©dio < Alto).

O script abaixo, mostra como voc√™ deve utilizar cada codificador. Escolha um e aplique na sua modelagem: 

```Py
# ...
x_data = df.iloc[:, 0:23].values
y_data = df.iloc[:, 23].values

from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer # Caso for utilizar o OneHotEncoder ou OrdinalEncoder

# ---------------------------------------------------------------
# 1. Label Encoder:
labelEncoder = LabelEncoder()

for i in range(x_data.shape[1]):
    x_data[:, i] = labelEncoder.fit_transform(x_data[:, i])

# ---------------------------------------------------------------
# 2. One Hot Encoder:
oneEncoder = OneHotEncoder()

oneTransform = ColumnTransformer(transformers=[('OneHot', oneEncoder, objects)], remainder='passthrough')

x_data = oneTransform.fit_transform(x_data)
# Observa√ß√£o: Dependendo do tamanho da sua base, o script pode retornar um erro. Isso ocorre, porque esse codificador criar v√°rias colunas para suportar digita√ß√µes bin√°rias. Para prevenir isso, tente realizar uma redu√ß√£o da base, aplicando conceitos de amostragem da estat√≠stica-

# ---------------------------------------------------------------
# 3. Ordinal Encoder:
ordinalEncoder = OrdinalEncoder()

ordinalTransform = ColumnTransformer(transformers=[('Ordinal', ordinalEncoder, objects)], remainder='passthrough')

# ---------------------------------------------------------------
```

### 2.2 Escalonamento de Vari√°veis Num√©ricas
- **StandardScaler** ‚Üí distribui√ß√µes pr√≥ximas da normal (m√©dia 0, desvio padr√£o 1).
- **MinMaxScaler** ‚Üí distribui√ß√µes n√£o normais (normaliza para [0,1]).
- Observa√ß√£o: nem todos os modelos precisam de escalonamento (√°rvores de decis√£o e random forest geralmente n√£o).

O script abaixo, mostra como voc√™ deve utilizar cada escalonador. Escolha um e aplique na sua modelagem: 

```Py
# ...
x_data = df.iloc[:, 0:23].values
y_data = df.iloc[:, 23].values

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# ---------------------------------------------------------------
# 1. Standard Scaler:
scalerStandard = StandardScaler()

x_data = scalerStandard.fit_transform(x_data)

# ---------------------------------------------------------------
# 2. Min Max Scaler:
scalerMinMax = MinMaxScaler()

x_data = scalerMinMax.fit_transform(x_data)


```


### 2.3 Dados Desbalanceados
- **Oversampling** ‚Üí aumenta a classe minorit√°ria (ex.: SMOTE).
- **Undersampling** ‚Üí reduz a classe majorit√°ria.

O script abaixo, mostra como voc√™ deve utilizar cada t√©cnica de tratar dados desbalanceados. Escolha um e aplique na sua modelagem: 

```Py
# ...
x_data = df.iloc[:, 0:23].values
y_data = df.iloc[:, 23].values

from imblearn.over_sampling import SMOTE

# ---------------------------------------------------------------
# 1. Oversampling:
smt = SMOTE(sampling_strategy = 'minority')

x_data, y_data = smt.fit_resample(x_data, y_data)

# 2. Undersampling:
tomek = TomekLinks(sampling_strategy = 'majority')

x_data, y_data = tomek.fit_resample(x_data, y_data)

```
---

## 3. Modelos de Machine Learning

Escolha de acordo com **tamanho da base, complexidade do problema e interpretabilidade desejada**.

### 3.1 Modelos Simples e Interpret√°veis
- Regress√£o Log√≠stica;
- Naive Bayes;
- √Årvores de Decis√£o;

```Py
# ...
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
```

### 3.2 Modelos Robustos (base m√©dia)
- Random Forest;
- Gradient Boosting (XGBoost, LightGBM, CatBoost) 

```Py
# ...
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
```

### 3.3 Modelos Complexos (grandes bases / alta dimensionalidade)
- SVM ‚Üí bom para dados de m√©dia a alta complexidade; pesado em grandes bases.
- Redes Neurais ‚Üí recomendadas para problemas complexos com grande volume de dados (imagens, texto, s√©ries temporais).

```Py
# ...
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
```

---

<div align='center'>
    <a href="Exercicio.md">Exerc√≠cio</a>
</div>
